{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWpRdOryy7dM",
        "outputId": "3a1651d6-6dc0-49c6-813a-7c8bef9a2b1e"
      },
      "source": [
        "This notebook was used to perform inference on unlabelled patent claims, and was run on Google colab. The best performing NLP model was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive on Google Colab to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OUYO8_bZ9YT"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import TFXLNetModel, XLNetTokenizer, XLNetForSequenceClassification, AdamW\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1oGARJlQdXQ-",
        "outputId": "44e290fd-c67f-4ab6-f942-b11cf0b63c4c"
      },
      "outputs": [],
      "source": [
        "# Identify and specify GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHZ5Ay2WGhBx"
      },
      "source": [
        "Combine unlabelled data into a single file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eLt2m2OxGsd"
      },
      "outputs": [],
      "source": [
        "df_1 = pd.read_excel(r'/content/drive/MyDrive/data/unlabeled_data/all_unlabelled_data_1.xlsx')\n",
        "df_2 = pd.read_excel(r'/content/drive/MyDrive/data/unlabeled_data/all_unlabelled_data_2.xlsx')\n",
        "df_3 = pd.read_excel(r'/content/drive/MyDrive/data/unlabeled_data/all_unlabelled_data_3.xlsx')\n",
        "\n",
        "combined_df = pd.concat([df_1, df_2, df_3], ignore_index=True)\n",
        "\n",
        "with open('all_unlabeled_claims.pkl', 'wb') as file:\n",
        "    pickle.dump(combined_df, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADteZl0zr8xU"
      },
      "source": [
        "Import claims data and prepare them to be inference ready (tokenizing and Tensor dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDRheS70lb87"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Load the object from the pickle file\n",
        "with open(r'/content/drive/My Drive/data/unlabeled_data/all_unlabeled_claims.pkl', 'rb') as file:\n",
        "    data = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruD21_U1nUOf"
      },
      "outputs": [],
      "source": [
        "# Function to prepare df input data to be inference-ready\n",
        "\n",
        "def df_to_tensor(data):\n",
        "\n",
        "  # Extract patent claims and prepare for inference\n",
        "\n",
        "  inputs = data['Text']\n",
        "  inputs = [sentence + \" [SEP] [CLS]\" for sentence in inputs]                      # Special tokens to be added to end of sentences for XLNet\n",
        "\n",
        "  # Initialize the tokenizer and convert text into tokens that correspond to XLNet's vocabulary\n",
        "  tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased',do_lower_case = True)\n",
        "  tokenised_inputs = [tokenizer.tokenize(sent) for sent in inputs]\n",
        "\n",
        "  MAX_LEN = 256\n",
        "\n",
        "  # Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenised_inputs]\n",
        "\n",
        "  # Pad our input tokens\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "  for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "  # Convert data into torch tensors, the required datatype for the model\n",
        "\n",
        "  inputs = torch.tensor(input_ids)\n",
        "  masks = torch.tensor(attention_masks)\n",
        "\n",
        "  input_data = TensorDataset(inputs,masks)\n",
        "\n",
        "  return input_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9bYg2bhsFgs"
      },
      "source": [
        "Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCitdU1xqi1F"
      },
      "outputs": [],
      "source": [
        "def inference(input_data,model,bs):\n",
        "  input_dataloader = DataLoader(input_data,batch_size = bs)\n",
        "\n",
        "  pred_flat_all = []\n",
        "\n",
        "  for batch in input_dataloader:\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask = batch\n",
        "      # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "      with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "          output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "          logits = output.logits\n",
        "\n",
        "      # Move logits to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "\n",
        "      pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "      pred_flat_all.append(pred_flat)\n",
        "\n",
        "  return pred_flat_all\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The inference code block below is designed to be able to run across multiple sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdA28KapAQ6s"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Imports completed inference from previous session\n",
        "with open(r'/content/drive/My Drive/data/all_labelled_claims.pkl', 'rb') as file:\n",
        "    labels = pickle.load(file)\n",
        "\n",
        "\n",
        "# Activate this line of code if running this block for the first time\n",
        "#labels = []\n",
        "\n",
        "\n",
        "# Imports inference model\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/Trained Models/XLNet/2e-05_0.01_32'\n",
        "model = XLNetForSequenceClassification.from_pretrained(model_path,num_labels = 2)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Find row where last iteration stopped\n",
        "start_loc = len(labels)\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "# Continue loop of inference\n",
        "try:\n",
        "  while start_loc < len(data):\n",
        "    print(start_loc)\n",
        "    data_subset = data.loc[start_loc:start_loc+batch_size - 1]\n",
        "\n",
        "    input_data = df_to_tensor(data_subset)\n",
        "\n",
        "    pred = inference(input_data,model,batch_size)\n",
        "\n",
        "    pred = pred[0].tolist()\n",
        "\n",
        "    labels = labels + pred\n",
        "\n",
        "    start_loc = start_loc + batch_size\n",
        "    with open(r'/content/drive/MyDrive/data/all_labelled_claims.pkl', 'wb') as file:\n",
        "        pickle.dump(labels, file)\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
